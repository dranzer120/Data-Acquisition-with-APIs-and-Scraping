{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dranzer120/Data-Acquisition-with-APIs-and-Scraping/blob/main/Lyrics_Scrape_Solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d44158d0",
      "metadata": {
        "id": "d44158d0"
      },
      "source": [
        "## Instructor Note\n",
        "\n",
        "This file is now superseded by `API and Scrape - Solutions.ipynb`, but I'm keeping it in the repo for posterity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3e39063",
      "metadata": {
        "id": "f3e39063"
      },
      "source": [
        "# ADS 509 Module 1 Part 2: Scraping Lyrics\n",
        "\n",
        "This notebook holds part two of the assignment for Module 1 in ADS 509, Applied Text Mining. Work through this notebook, writing code and answering questions where required.\n",
        "\n",
        "For this assignment you have chosen two musical artists who have at least 100,000 Twitter followers and 20 songs with lyrics on AZLyrics.com. In this part of the assignment we pull the artists' songs (or a subset) and store them in text files.\n",
        "\n",
        "The lyrics for an artist on AZLyrics can be accessed via a link like this: https://www.azlyrics.com/r/robyn.html. You will need to find the links for your two artists."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6e65f73",
      "metadata": {
        "id": "c6e65f73"
      },
      "source": [
        "## General Assignment Instructions\n",
        "\n",
        "These instructions are included in every assignment, to remind you of the coding standards for the class. Feel free to delete this cell after reading it.\n",
        "\n",
        "One sign of mature code is conforming to a style guide. We recommend the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html). If you use a different style guide, please include a cell with a link.\n",
        "\n",
        "Your code should be relatively easy-to-read, sensibly commented, and clean. Writing code is a messy process, so please be sure to edit your final submission. Remove any cells that are not needed or parts of cells that contain unnecessary code. Remove inessential `import` statements and make sure that all such statements are moved into the designated cell.\n",
        "\n",
        "Make use of non-code cells for written commentary. These cells should be grammatical and clearly written. In some of these cells you will have questions to answer. The questions will be marked by a \"Q:\" and will have a corresponding \"A:\" spot for you. *Make sure to answer every question marked with a `Q:` for full credit.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "abe420bf",
      "metadata": {
        "id": "abe420bf"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import os\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "79dc6c1a",
      "metadata": {
        "id": "79dc6c1a"
      },
      "outputs": [],
      "source": [
        "# Use this cell for any import statements you add\n",
        "import re\n",
        "import shutil\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "bd7d80ea",
      "metadata": {
        "id": "bd7d80ea"
      },
      "outputs": [],
      "source": [
        "artists = {'robyn':\"https://www.azlyrics.com/r/robyn.html\",\n",
        "           'cher':\"https://www.azlyrics.com/c/cher.html\"}\n",
        "# we'll use this dictionary to hold both the artist name and the link on AZlyrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57899ef1",
      "metadata": {
        "id": "57899ef1"
      },
      "source": [
        "## A Note on Rate Limiting\n",
        "\n",
        "The lyrics site, www.azlyrics.com, does not have an explicit maximum on number of requests in any one time, but in our testing it appears that too many requests in too short a time will cause the site to stop returning lyrics pages. (Entertainingly, the page that gets returned seems to only have the song title to [a Tom Jones song](https://www.azlyrics.com/lyrics/tomjones/itsnotunusual.html).)\n",
        "\n",
        "Whenever you call `requests.get` to retrieve a page, put a `time.sleep(5 + 10*random.random())` on the next line. This will help you not to get blocked. If you _do_ get blocked, which you can identify if the returned pages are not correct, just request a lyrics page through your browser. You'll be asked to perform a CAPTCHA and then your requests should start working again.\n",
        "\n",
        "## Part 1: Finding Links to Songs Lyrics\n",
        "\n",
        "That general artist page has a list of all songs for that artist with links to the individual song pages.\n",
        "\n",
        "Q: Take a look at the `robots.txt` page on www.azlyrics.com. (You can read more about these pages [here](https://developers.google.com/search/docs/advanced/robots/intro).) Is the scraping we are about to do allowed or disallowed by this page? How do you know?\n",
        "\n",
        "A: <!-- Delete this comment and put your answer here. --> This scraping is allowed. The `robots.txt` file disallows scraping the lyricsdb and song portions of the file hierarchy. Everything else, including the artist and lyric pages, is allowed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a3cf6e42",
      "metadata": {
        "id": "a3cf6e42"
      },
      "outputs": [],
      "source": [
        "# Let's set up a dictionary of lists to hold our links\n",
        "lyrics_pages = defaultdict(list)\n",
        "\n",
        "for artist, artist_page in artists.items() :\n",
        "    r = requests.get(artist_page)\n",
        "    time.sleep(5 + 10*random.random())\n",
        "\n",
        "    # Test for good request and soupify\n",
        "    if r.status_code == 200 :\n",
        "        soup = BeautifulSoup(r.text,\"html.parser\")\n",
        "\n",
        "        # many ways to do this, but here's a quick way to get\n",
        "        # a tags with href values matching the lyrics pages\n",
        "        for page_link in soup.find_all('a',href=re.compile(\"/lyrics/\")) :\n",
        "            lyrics_pages[artist].append(page_link.attrs['href'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28f91355",
      "metadata": {
        "id": "28f91355"
      },
      "source": [
        "Let's make sure we have enough lyrics pages to scrape."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lyrics_pages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnFTb_T2vRku",
        "outputId": "d5b6cb28-bc3c-4580-f93f-659888d98c7c"
      },
      "id": "EnFTb_T2vRku",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(list, {})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6b543cc4",
      "metadata": {
        "id": "6b543cc4"
      },
      "outputs": [],
      "source": [
        "for artist, lp in lyrics_pages.items() :\n",
        "    assert(len(set(lp)) > 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "598b81a0",
      "metadata": {
        "id": "598b81a0"
      },
      "outputs": [],
      "source": [
        "# Let's see how long it's going to take to pull these lyrics\n",
        "# if we're waiting `5 + 10*random.random()` seconds\n",
        "for artist, links in lyrics_pages.items() :\n",
        "    print(f\"For {artist} we have {len(links)}.\")\n",
        "    print(f\"The full pull will take for this artist will take {round(len(links)*10/3600,2)} hours.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e198019",
      "metadata": {
        "id": "7e198019"
      },
      "source": [
        "## Part 2: Pulling Lyrics\n",
        "\n",
        "Now that we have the links to our lyrics pages, let's go scrape them! Here are the steps for this part.\n",
        "\n",
        "1. Create an empty folder in our repo called \"lyrics\".\n",
        "1. Iterate over the artists in `lyrics_pages`.\n",
        "1. Create a subfolder in lyrics with the artist's name. For instance, if the artist was Cher you'd have `lyrics/cher/` in your repo.\n",
        "1. Iterate over the pages.\n",
        "1. Request the page and extract the lyrics from the returned HTML file using BeautifulSoup.\n",
        "1. Use the function below, `generate_filename_from_url`, to create a filename based on the lyrics page, then write the lyrics to a text file with that name.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6b4e67bf",
      "metadata": {
        "id": "6b4e67bf"
      },
      "outputs": [],
      "source": [
        "def generate_filename_from_link(link) :\n",
        "\n",
        "    if not link :\n",
        "        return None\n",
        "\n",
        "    # drop the http or https and the html\n",
        "    name = link.replace(\"https\",\"\").replace(\"http\",\"\")\n",
        "    name = link.replace(\".html\",\"\")\n",
        "\n",
        "    name = name.replace(\"/lyrics/\",\"\")\n",
        "\n",
        "    # Replace useless chareacters with UNDERSCORE\n",
        "    name = name.replace(\"://\",\"\").replace(\".\",\"_\").replace(\"/\",\"_\")\n",
        "\n",
        "    # tack on .txt\n",
        "    name = name + \".txt\"\n",
        "\n",
        "    return(name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ef15e439",
      "metadata": {
        "id": "ef15e439"
      },
      "outputs": [],
      "source": [
        "# Make the lyrics folder here. If you'd like to practice your programming, add functionality\n",
        "# that checks to see if the folder exists. If it does, then \"unlink\" it. Then create a new one.\n",
        "\n",
        "if os.path.isdir(\"lyrics\") :\n",
        "    shutil.rmtree(\"lyrics/\")\n",
        "\n",
        "os.mkdir(\"lyrics\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "8498a674",
      "metadata": {
        "id": "8498a674"
      },
      "outputs": [],
      "source": [
        "url_stub = \"https://www.azlyrics.com\"\n",
        "start = time.time()\n",
        "\n",
        "total_pages = 0\n",
        "\n",
        "for artist in lyrics_pages :\n",
        "\n",
        "    os.mkdir(\"lyrics/\" + artist)\n",
        "    pages = 0\n",
        "\n",
        "    for page in lyrics_pages[artist] :\n",
        "\n",
        "        page_name = page.replace(\"/lyrics/\",\"\").replace(\".html\",\"\")\n",
        "        print(f\"Requesting: {page_name}\")\n",
        "\n",
        "        # make actual URL\n",
        "        url = url_stub + page\n",
        "\n",
        "        # request\n",
        "        r = requests.get(url)\n",
        "        time.sleep(5 + 10*random.random()) # Put in a random wait\n",
        "\n",
        "        pages += 1\n",
        "\n",
        "        if r.status_code != 200 :\n",
        "            print(f\"Got a bad status code ({r.status_code}) on {page}.\")\n",
        "        else :\n",
        "            soup = BeautifulSoup(r.text,\"html.parser\")\n",
        "\n",
        "            output_filename = \"lyrics/\" + artist + \"/\" + generate_filename_from_link(page)\n",
        "\n",
        "            # First let's get the song title.\n",
        "            for item in soup.find_all(\"b\") :\n",
        "                if '\"' in item.text :\n",
        "                    title = item.text\n",
        "                    break\n",
        "                    # First one with quotes is song. Second is album.\n",
        "\n",
        "            # Let's get the lyrics\n",
        "            hit_ringtone = False\n",
        "            for item in soup.find_all(\"div\") :\n",
        "\n",
        "                # Lyrics are the div after the ringtone\n",
        "                if hit_ringtone :\n",
        "                    break\n",
        "\n",
        "                if \"class\" in item.attrs :\n",
        "                    if \"ringtone\" in item.attrs[\"class\"] :\n",
        "                        hit_ringtone = True\n",
        "\n",
        "            lyrics = item.text\n",
        "\n",
        "            if title == '\"It\\'s Not Unusual\"' :\n",
        "                print(\"Oops, looks like we went too fast. Getting Tom Jones back.\")\n",
        "                time.sleep(360 + random.random()*80) # see if a long sleep works\n",
        "\n",
        "            with open(output_filename,'w') as ofile :\n",
        "                ofile.write(title + \"\\n\\n\")\n",
        "                ofile.write(lyrics)\n",
        "\n",
        "        # Comment out the next line for the full pull.\n",
        "        #if pages == 10 :\n",
        "        #x    break\n",
        "\n",
        "    # Sleep between artists\n",
        "    time.sleep(60)\n",
        "    total_pages += pages\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b84948bf",
      "metadata": {
        "id": "b84948bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3591034a-492b-4c3f-8f60-15fcc0c5ab9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total run time was 0.0 hours.\n"
          ]
        }
      ],
      "source": [
        "print(f\"Total run time was {round((time.time() - start)/3600,2)} hours.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}